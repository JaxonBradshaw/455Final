{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaxonBradshaw/455Final/blob/main/FinalProject455.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyLDAvis\n",
        "# !pip install pyLDAvis.gensim\n",
        "# !pip install logging\n",
        "# !pip install -U pip setuptools wheel\n",
        "# !pip install -U spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# !pip install spacy\n",
        "# !pip install gower"
      ],
      "metadata": {
        "id": "lj_akTMEHWql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Data Collection"
      ],
      "metadata": {
        "id": "SbpTnTFuBfdt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZG-w2B0W6Wl"
      },
      "outputs": [],
      "source": [
        "def collectData(url, numberOfRows, brand):\n",
        "  import requests\n",
        "  import pandas as pd\n",
        "  import json\n",
        "  from bs4 import BeautifulSoup\n",
        "  df = pd.DataFrame(columns = ['name', 'gender', 'silhouette', 'releaseDate', 'retailPrice', 'estimatedMarketValue', 'story', 'image'])\n",
        "  numberOfPages = int(numberOfRows / 100)\n",
        "\n",
        "  #Filter through pages of the api data, collecting 100 shoes at a time\n",
        "  for i in range(numberOfPages):\n",
        "\n",
        "    #Include a filter by brand (We only want Jordans)\n",
        "    querystring = {\"limit\":\"100\", 'page': str(i + 1), 'brand': brand}\n",
        "\n",
        "    headers = {\n",
        "          #This API only allows 200 calls a month (on the free version)\n",
        "          #Update this key to a new account when you run out of calls\n",
        "          'x-rapidapi-host': 'the-sneaker-database.p.rapidapi.com',\n",
        "          'x-rapidapi-key': 'c14d7bed93msh87c86d4d1ac7f06p174d68jsnbd2166c5d18b'\n",
        "        }\n",
        "\n",
        "    #Request the URL, if it doesn't work, return the page number that we are on\n",
        "    try: \n",
        "      response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
        "      json_data = json.loads(response.text)\n",
        "    except: \n",
        "      print(i + 1)\n",
        "\n",
        "    #Filter through the results and only add shoes that have both an image and a story\n",
        "    #This will result in returning less rows than the desired \"numberOfRows\" variable \n",
        "    for shoe in json_data['results']:\n",
        "      if shoe['image']['original'] != '' and shoe['story'] != '' and shoe['retailPrice'] != 0:\n",
        "        df.loc[shoe['name']] = shoe['name'], shoe['gender'], shoe['silhouette'], shoe['releaseDate'], shoe['retailPrice'], shoe['estimatedMarketValue'], shoe['story'], shoe['image']['original']\n",
        "    \n",
        "  df.set_index('name', inplace=True)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collectData2(): \n",
        "  import pandas as pd\n",
        "\n",
        "  #Update with your own path if you don't want to pull your own data!\n",
        "  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/finalProjectImageData.csv')\n",
        "  \n",
        "  df.set_index('name', inplace=True)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "IslCE8beti9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Binning Groups\n"
      ],
      "metadata": {
        "id": "qGZ0lHEWBu2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bin_gender(df, col, percent=0.05):\n",
        "  import pandas as pd\n",
        "\n",
        "  df.loc[df[col] == 'Mens', col] = 'men'\n",
        "  df.loc[df[col] == 'infant', col] = 'toddler'\n",
        "\n",
        "  for group, count in df[col].value_counts().iteritems():\n",
        "    if count / len(df) < percent:\n",
        "      df.loc[df[col] == group, col] = 'Other'\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "K7Zzei7tB2fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bin_silhouette(df, col, percent=0.05):\n",
        "  df[[col]] = df[[col]].astype(str) \n",
        "  for shoe in df.itertuples(): \n",
        "    if shoe[2].find('Jordan 1 ') != -1 or shoe[2].find('Air Force 1') != -1:\n",
        "      df.loc[shoe[0], col] = 'Air Jordan 1' \n",
        "\n",
        "    if shoe[2].find('jordan 6') != -1:\n",
        "      df.loc[shoe[0], col] = 'Air Jordan 6'\n",
        "\n",
        "  for group, count in df[col].value_counts().iteritems():\n",
        "    if count / len(df) < percent:\n",
        "      df.loc[df[col] == group, col] = 'Other'\n",
        "\n",
        "  return df\n",
        "  "
      ],
      "metadata": {
        "id": "TxjrldgD2mhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Fix Dates"
      ],
      "metadata": {
        "id": "5yfk99b_do9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_dates(df, col):\n",
        "  import datetime \n",
        "  from dateutil.relativedelta import relativedelta\n",
        "  from datetime import date\n",
        "  \n",
        "  date = datetime.datetime.strptime('2022-01-01', \"%Y-%m-%d\")\n",
        "  dates_list = []\n",
        "  compared_dates = []\n",
        "\n",
        "  #loop through each row of the specified column\n",
        "  for row in df[col] :\n",
        "    #make two lists of the date in the row and the date to compare it to\n",
        "    row = row[:10]\n",
        "    row_date = datetime.datetime.strptime(row, \"%Y-%m-%d\")\n",
        "    difference = date - row_date\n",
        "    dates_list.append(difference.days)\n",
        "\n",
        "  #replace it with the difference dataframe in days\n",
        "  df[col] = dates_list\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "3XZjV9rtdstS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Adjust for Skewness"
      ],
      "metadata": {
        "id": "48rqbvEyfG74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_skewness(df): \n",
        "  import numpy as np\n",
        "\n",
        "  # Cast both as type int (some values are weird)\n",
        "  df[['estimatedMarketValue']] = df[['estimatedMarketValue']].astype(int) \n",
        "  df[['retailPrice']] = df[['retailPrice']].astype(int) \n",
        "\n",
        "  df['estimatedMarketValue'] = np.log(df['estimatedMarketValue'] + 1)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "V4HEf59mRuZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Text Analytics\n"
      ],
      "metadata": {
        "id": "TkSckAp5V19r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_to_words(sentences):\n",
        "  import sys, re, numpy as np\n",
        "  import pandas as pd\n",
        "  from pprint import pprint\n",
        "  import gensim, spacy, logging, warnings, en_core_web_sm\n",
        "  import gensim.corpora as corpora\n",
        "  from gensim.utils import lemmatize, simple_preprocess\n",
        "  from gensim.models import CoherenceModel\n",
        "  import matplotlib.pyplot as plt\n",
        "  from nltk.corpus import stopwords\n",
        "  import nltk\n",
        "\n",
        "  for sent in sentences:\n",
        "    sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
        "    sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
        "    sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
        "    sent = re.sub('\\\"', \"\", sent) # remove double quotes\n",
        "    sent = gensim.utils.simple_preprocess(str(sent), deacc=True)\n",
        "    yield(sent)"
      ],
      "metadata": {
        "id": "-8OWBWFpcoBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_words(texts, stop_words, bigram_mod, trigram_mod, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "  import sys, re, numpy as np\n",
        "  import pandas as pd\n",
        "  from pprint import pprint\n",
        "  import gensim, spacy, logging, warnings, en_core_web_sm\n",
        "  import gensim.corpora as corpora\n",
        "  from gensim.utils import lemmatize, simple_preprocess\n",
        "  from gensim.models import CoherenceModel\n",
        "  import matplotlib.pyplot as plt\n",
        "  from nltk.corpus import stopwords\n",
        "  import nltk\n",
        "  \"\"\"Remove Stopwords, Form Bigrams, Trigrams and perform Lemmatization\"\"\"\n",
        "  texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "  texts = [bigram_mod[doc] for doc in texts]\n",
        "  texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "  texts_out = []\n",
        "  nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])    # Load spacy, but we don't need the parser or NER (named entity extraction) modules\n",
        "\n",
        "  for sent in texts:\n",
        "    doc = nlp(\" \".join(sent))\n",
        "    texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "\n",
        "  # remove stopwords once more after lemmatization\n",
        "  texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]\n",
        "  return texts_out"
      ],
      "metadata": {
        "id": "WZXLURcgcn07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_analytics(df):\n",
        "  import sys, re, numpy as np\n",
        "  import pandas as pd\n",
        "  from pprint import pprint\n",
        "  import gensim, spacy, logging, warnings, en_core_web_sm\n",
        "  import gensim.corpora as corpora\n",
        "  from gensim.utils import lemmatize, simple_preprocess\n",
        "  from gensim.models import CoherenceModel\n",
        "  import matplotlib.pyplot as plt\n",
        "  from nltk.corpus import stopwords\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "  stop_words = stopwords.words('english')\n",
        "\n",
        "  stop_words.extend(['air', 'heel', 'upper', 'leather', 'black', 'tongue', 'white', 'feature', 'color', 'outsole', 'retro',\n",
        "                     'midsole', 'sneaker', 'design', 'rubber', 'red', 'branding', 'contrast', 'overlay', 'make', 'top', 'collar',\n",
        "                     'blue', 'shoe', 'colorway', 'release', 'finish', 'jumpman', 'signature', 'detail', 'logo', 'tag', 'swoosh',\n",
        "                     'low', 'high', 'inspire', 'accent', 'nike', 'original', 'silhouette', 'classic', 'base','flap', 'foam', 'wing',\n",
        "                     'lateral', 'mid', 'include', 'hue', 'green', 'suede', 'visible', 'panel', 'weave', 'unit', 'silver', 'embroider',\n",
        "                     'deliver', 'take', 'mesh', 'scheme', 'combine', 'side', 'sole', 'metallic', 'gold', 'look', 'nubuck', 'element',\n",
        "                     'hit', 'textile', 'yellow', 'construct', 'cushioning', 'woman', 'wear', 'iconic', 'ride', 'zoom', 'print',\n",
        "                     'translucent', 'dark', 'grey', 'patent', 'purple', 'toe', 'forefoot', 'pop', 'underfoot', 'update', 'build',\n",
        "                     'tumble', 'pattern', 'apply', 'give', 'offer', 'quarter', 'pink', 'complete', 'vibrant', 'royal', 'ankle',\n",
        "                     'perforate', 'midfoot', 'lace', 'cupsole', 'synthetic', 'version', 'stitch', 'icon', 'appear', 'provide', 'first', \n",
        "                     'brand', 'university', 'lightweight', 'strap', 'box', 'style', 'celebrate', 'support', 'two_tone', 'edition',\n",
        "                     'mudguard', 'traditional', 'game','model', 'reflective', 'kid', 'fit', 'premium', 'equip', 'material', 'encapsulate',\n",
        "                     'cover', 'responsive', 'pair', 'match', 'court', 'present', 'highlight', 'additional', 'bright'])\n",
        "\n",
        "  data = df.story.values.tolist()\n",
        "\n",
        "  data_words = list(sent_to_words(data))\n",
        "\n",
        "  bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "  trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "  bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "  trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "  data_ready = process_words(data_words, stop_words, bigram_mod, trigram_mod)\n",
        "\n",
        "  id2word = corpora.Dictionary(data_ready)\n",
        "\n",
        "  corpus = [id2word.doc2bow(text) for text in data_ready]\n",
        "\n",
        "  # CODE TO FIND NUMBER OF TOPICS\n",
        "\n",
        "  #df_fit = pd.DataFrame(columns=['topics', 'perplexity', 'coherence'])\n",
        "\n",
        "  # for n in range(3, 10):\n",
        "  #   lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "  #                                               id2word=id2word,\n",
        "  #                                               num_topics=n,\n",
        "  #                                               random_state=12345,\n",
        "  #                                               chunksize=20,\n",
        "  #                                               passes=10,\n",
        "  #                                               per_word_topics=True)\n",
        "\n",
        "  # # # #   # Generate fit metrics\n",
        "  #   coherence_model_lda = CoherenceModel(model=lda_model, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
        "  # # # #   # Add metrics to df_fit\n",
        "  #   df_fit.loc[n - 3] = [n, round(lda_model.log_perplexity(corpus), 3), round(coherence_model_lda.get_coherence(), 3)]\n",
        "\n",
        "\n",
        "  # print(df_fit)\n",
        "\n",
        "  topics = 3\n",
        "\n",
        "  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                              id2word=id2word,\n",
        "                                              num_topics=topics,\n",
        "                                              random_state=12345,\n",
        "                                              chunksize=20,\n",
        "                                              passes=10,\n",
        "                                              per_word_topics=True)\n",
        "\n",
        "  ldatopics = lda_model.show_topics(formatted=False)\n",
        "\n",
        "  num_topics = len(lda_model.get_topics()) # store the number of topics from the last model\n",
        "  for col in range(num_topics): # generate a new column for each topic\n",
        "    df[f'topic_{col + 1}'] = 0.0\n",
        "\n",
        "  # Store the topic score and dominant topic\n",
        "  for i, words in enumerate(data_ready):\n",
        "    doc = lda_model[id2word.doc2bow(words)] # generate a corpus for this document set of words\n",
        "\n",
        "    for j, score in enumerate(doc[0]): # for each document in the corpus\n",
        "      # Get the topic score and store it in the appropriate column\n",
        "      df.iat[i, (len(df.columns) - ((num_topics) - score[0]))] = score[1]\n",
        "\n",
        "\n",
        "  # Code to figure out stopwords that need to be removed!\n",
        "\n",
        "  # from collections import Counter\n",
        "  # from matplotlib import pyplot as plt\n",
        "  # from wordcloud import WordCloud, STOPWORDS\n",
        "  # import matplotlib.colors as mcolors\n",
        "  # topics = lda_model.show_topics(formatted=False)\n",
        "  # data_flat = [w for w_list in data_ready for w in w_list]\n",
        "  # counter = Counter(data_flat)\n",
        "\n",
        "  # out = []\n",
        "  # for i, topic in topics:\n",
        "  #     for word, weight in topic:\n",
        "  #         out.append([word, i , weight, counter[word]])\n",
        "\n",
        "  # df_words = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])\n",
        "\n",
        "  # # Plot Word Count and Weights of Topic Keywords\n",
        "  # fig, axes = plt.subplots(1, 3, figsize=(20,7), sharey=True, dpi=160)\n",
        "  # cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
        "  # for i, ax in enumerate(axes.flatten()):\n",
        "  #     ax.bar(x='word', height=\"word_count\", data=df_words.loc[df_words.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
        "  #     ax_twin = ax.twinx()\n",
        "  #     ax_twin.bar(x='word', height=\"importance\", data=df_words.loc[df_words.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
        "  #     ax.set_ylabel('Word Count', color=cols[i])\n",
        "  #     # ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
        "  #     ax.set_title('Topic: ' + str(i + 1), color=cols[i], fontsize=16)\n",
        "  #     ax.tick_params(axis='y', left=False)\n",
        "  #     ax.set_xticklabels(df_words.loc[df_words.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
        "  #     ax.legend(loc='upper center'); ax_twin.legend(loc='upper right')\n",
        "\n",
        "  # fig.tight_layout(w_pad=2)\n",
        "  # fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)\n",
        "  # plt.show()\n",
        "\n",
        "  return df\n",
        "\n"
      ],
      "metadata": {
        "id": "-iuPB40VV-8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_calc(df):\n",
        "  import nltk\n",
        "  from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "  # Word lists and lexicons in nltk: https://www.nltk.org/howto/corpus.html#word-lists-and-lexicons\n",
        "  nltk.download('vader_lexicon')\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  sia.polarity_scores(\"This is a really great story!\")\n",
        "\n",
        "  df['sentiment_overall'] = 0.0\n",
        "  df['sentiment_neg'] = 0.0\n",
        "  df['sentiment_neu'] = 0.0\n",
        "  df['sentiment_pos'] = 0.0\n",
        "\n",
        "  for row in df.itertuples():\n",
        "    sentiment = sia.polarity_scores(row[6])\n",
        "    df.loc[row[0], 'sentiment_overall'] = sentiment['compound']\n",
        "    df.loc[row[0], 'sentiment_neg'] = sentiment['neg']\n",
        "    df.loc[row[0], 'sentiment_neu'] = sentiment['neu']\n",
        "    df.loc[row[0], 'sentiment_pos'] = sentiment['pos']\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "BTNOtWp1QygT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_count(df):\n",
        "\n",
        "  df['story_length'] = 0\n",
        "\n",
        "  for row in df.itertuples(): \n",
        "    df.loc[row[0], 'story_length'] = len(df.loc[row[0], 'story'])\n",
        "  \n",
        "  return df"
      ],
      "metadata": {
        "id": "AMMLBkj1SEzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_entities(df):\n",
        "  # Identifies the entities that make up the text (names, organizations, countries, etc.)\n",
        "\n",
        "  df['org_count_text'] = 0\n",
        "  df['gpe_count_text'] = 0\n",
        "  df['product_count_text'] = 0\n",
        "  df['loc_count_text'] = 0\n",
        "  df['date_count_text'] = 0\n",
        "  df['ordinal_count_text'] = 0\n",
        "  df['money_count_text'] = 0\n",
        "  df['person_count_text'] = 0\n",
        "\n",
        "  import spacy\n",
        "  from spacy import displacy\n",
        "  for row in df.itertuples():\n",
        "    NER = spacy.load(\"en_core_web_sm\")\n",
        "    text1= NER(row[6])\n",
        "\n",
        "    for word in text1.ents:\n",
        "      if word.label_ == 'ORG':\n",
        "        df.loc[row[0], 'org_count_text'] = df.loc[row[0], 'org_count_text'] + 1\n",
        "      elif word.label_ == 'GPE':\n",
        "        df.loc[row[0], 'gpe_count_text'] = df.loc[row[0], 'gpe_count_text'] + 1\n",
        "      elif word.label_ == 'PRODUCT':\n",
        "        df.loc[row[0], 'product_count_text'] = df.loc[row[0], 'product_count_text'] + 1\n",
        "      elif word.label_ == 'LOC':\n",
        "        df.loc[row[0], 'loc_count_text'] = df.loc[row[0], 'loc_count_text'] + 1\n",
        "      elif word.label_ == 'DATE':\n",
        "        df.loc[row[0], 'date_count_text'] = df.loc[row[0], 'date_count_text'] + 1\n",
        "      elif word.label_ == 'ORDINAL':\n",
        "        df.loc[row[0], 'ordinal_count_text'] = df.loc[row[0], 'ordinal_count_text'] + 1\n",
        "      elif word.label_ == 'MONEY':\n",
        "        df.loc[row[0], 'money_count_text'] = df.loc[row[0], 'money_count_text'] + 1\n",
        "      elif word.label_ == 'PERSON':\n",
        "        df.loc[row[0], 'person_count_text'] = df.loc[row[0], 'person_count_text'] + 1\n",
        "    \n",
        "    #displacy.render(text1,style=\"ent\",jupyter=True)\n",
        "\n",
        "  # Drops story and images once the image processing and text anayltics is complete\n",
        "  df.drop(['story', 'image'], axis=1, inplace=True)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "I62y7TjDc8Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Image Analytics"
      ],
      "metadata": {
        "id": "-VWtyVIrV56d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_analytics(df):\n",
        "  import json\n",
        "  import requests\n",
        "\n",
        "  df['color_variance'] = 0\n",
        "\n",
        "  for i, shoe in enumerate(df.itertuples()):\n",
        "    try: \n",
        "      image_url = shoe[7]\n",
        "\n",
        "      if i % 3 == 1:\n",
        "        api_key = 'acc_58e22a11bd859d6' \n",
        "        api_secret = '3f6951c2a8a851bf5e91199b141ca3d6'\n",
        "      elif i % 3 == 2: \n",
        "        api_key = 'acc_12406f00a60ea3d'\n",
        "        api_secret = '767a4f200acdfa7d96feaba046b72302'\n",
        "      else: \n",
        "        api_key = 'acc_1d66baefb178be0'\n",
        "        api_secret = '92278e0f8f4ed443e42526fd40b67801'\n",
        "      \n",
        "\n",
        "      response = requests.get(\n",
        "          'https://api.imagga.com/v2/colors?overall_count=7&separated_count=5&image_url=%s' % image_url,\n",
        "          auth=(api_key, api_secret))\n",
        "\n",
        "      json_data = response.json()\n",
        "\n",
        "      # Focus on colors of the image, not background colors\n",
        "      for foreground_color in json_data['result']['colors']['foreground_colors']:\n",
        "\n",
        "        # Better to use parent color instead of actual color so that we don't have as many columns\n",
        "        parent_color = foreground_color['closest_palette_color_parent']\n",
        "        percentage = foreground_color['percent']\n",
        "\n",
        "        # Create new columns/Add percentages\n",
        "        if (\"color_\" + parent_color) in df.columns: \n",
        "          df.loc[shoe[0], (\"color_\" + parent_color)] = percentage\n",
        "        else: \n",
        "          df[\"color_\" + parent_color] = 0\n",
        "          df.loc[shoe[0], (\"color_\" + parent_color)] = percentage\n",
        "      \n",
        "      # Add the color variance as well\n",
        "      color_variance = json_data['result']['colors']['color_variance']\n",
        "    \n",
        "      df.loc[shoe[0], 'color_variance'] = color_variance\n",
        "\n",
        "    except: \n",
        "      df.drop(shoe[0], inplace=True)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "q9qf0dx1WMw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Fill Missing Values"
      ],
      "metadata": {
        "id": "z6pZ6HxDfB-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def impute_reg(df, label):\n",
        "  from sklearn.experimental import enable_iterative_imputer\n",
        "  from sklearn.impute import IterativeImputer\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  import pandas as pd\n",
        "\n",
        "  # Dummy code first; categorical features not allowed\n",
        "  for col in df:\n",
        "    # We use this code for both regression and classification, so if we are using 'gender' as a label,\n",
        "    # we don't want to dummy code it\n",
        "    if not pd.api.types.is_numeric_dtype(df[col]) and (label != 'gender' or col != 'gender'):\n",
        "      df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "\n",
        "  # Scaling is unnecessary for regression-based imputation\n",
        "\n",
        "  # Save the gender values\n",
        "  if label == 'gender':\n",
        "    gender_column = df['gender'].values\n",
        "    df.drop(columns=['gender'], inplace=True)\n",
        "\n",
        "  imp = IterativeImputer(max_iter=10, random_state=12345)\n",
        "  df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
        "\n",
        "  # Replace the gender values after imputing\n",
        "  if label == 'gender':\n",
        "    df['gender'] = gender_column\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "2_x_AZ8lf9UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Feature Selection"
      ],
      "metadata": {
        "id": "HYa06jbXc3Wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_selection_variance(df, label=\"\", p=0.8):\n",
        "  from sklearn.feature_selection import VarianceThreshold\n",
        "  import pandas as pd\n",
        "  \n",
        "  if label != \"\":\n",
        "    X = df.drop(columns=[label])\n",
        "      \n",
        "  sel = VarianceThreshold(threshold=(p * (1 - p)))\n",
        "  sel.fit_transform(X)\n",
        "  \n",
        "  # Add the label back in after removing poor features\n",
        "  return df[sel.get_feature_names_out()].join(df[label])"
      ],
      "metadata": {
        "id": "jKHJciVAypz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Modeling"
      ],
      "metadata": {
        "id": "7_LFL6oK24YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_crossvalidate_reg(df, label, k=10, n=5, repeat=True):\n",
        "  import sklearn.linear_model as lm, sklearn.ensemble as se\n",
        "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
        "  import pandas as pd\n",
        "  from numpy import mean, std\n",
        "  from xgboost import XGBRegressor\n",
        "\n",
        "  X = df.drop(columns=[label])\n",
        "  y = df[label]\n",
        "\n",
        "  if repeat:\n",
        "    cv = RepeatedKFold(n_splits=k, n_repeats=n, random_state=12345)\n",
        "  else:\n",
        "    cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
        "\n",
        "  fit = {}\n",
        "  model = {}\n",
        "\n",
        "  model_lr = lm.LinearRegression()\n",
        "  model_ridge = lm.Ridge()\n",
        "  model_lasso = lm.Lasso(alpha=0.1)\n",
        "  model_ada = se.AdaBoostRegressor(random_state=12345, n_estimators=100)\n",
        "  model_xgb = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n",
        "  model_br = lm.BayesianRidge()\n",
        "  model_pr = lm.TweedieRegressor(power=1, link=\"log\") # Power=1 means this is a Poisson\n",
        "  model_igr = lm.TweedieRegressor(power=3) # Power=3 means this is an inverse Gamma\n",
        "               \n",
        "  fit['MLR'] = mean(cross_val_score(model_lr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
        "  fit['Ridge'] = mean(cross_val_score(model_ridge, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
        "  fit['Lasso'] = mean(cross_val_score(model_lasso, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
        "  fit['AdaBoost'] = mean(cross_val_score(model_ada, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
        "  fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
        "  fit['Bayesian'] = mean(cross_val_score(model_br, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
        "  fit['Poisson'] = mean(cross_val_score(model_pr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
        "  fit['Inverse'] = mean(cross_val_score(model_igr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
        "\n",
        "  model['MLR'] = model_lr\n",
        "  model['Ridge'] = model_ridge\n",
        "  model['Lasso'] = model_lasso\n",
        "  model['AdaBoost'] = model_ada\n",
        "  model['XGBoost'] = model_xgb\n",
        "  model['Bayesian'] = model_br\n",
        "  model['Poisson'] = model_pr\n",
        "  model['Inverse'] = model_igr\n",
        "\n",
        "  df_fit = pd.DataFrame({'R-squared':fit})\n",
        "  df_fit = df_fit.sort_values(by=['R-squared'], ascending=False)\n",
        "\n",
        "  print('Regression Models')\n",
        "  print(df_fit)\n",
        "\n",
        "  best_model = df_fit.index[0]\n",
        "  return model[best_model].fit(X, y)"
      ],
      "metadata": {
        "id": "VWpVPsDM20hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_crossvalidate_clf(df, label, k=10, n=5, repeat=True):\n",
        "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
        "  import pandas as pd\n",
        "  from numpy import mean, std\n",
        "  from xgboost import XGBClassifier\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  import sklearn.linear_model as lm, sklearn.ensemble as en\n",
        "\n",
        "  X = df.drop(columns=[label])\n",
        "  y = df[label]\n",
        "\n",
        "  if repeat:\n",
        "    cv = RepeatedKFold(n_splits=k, n_repeats=n, random_state=12345)\n",
        "  else:\n",
        "    cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
        "\n",
        "  fit = {}\n",
        "  model = {}\n",
        "\n",
        "  model_log = lm.LogisticRegression(max_iter=100)\n",
        "  model_knn = KNeighborsClassifier(n_neighbors=3)\n",
        "  model_ridge = lm.RidgeClassifier()\n",
        "  model_ada = en.AdaBoostClassifier(n_estimators=100, random_state=12345)\n",
        "  model_gb = en.GradientBoostingClassifier(random_state=12345)\n",
        "  model_xgb = XGBClassifier(objective = 'binary:logistic')\n",
        "\n",
        "  fit['Logistic'] = mean(cross_val_score(model_log, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
        "  fit['KNN'] = mean(cross_val_score(model_knn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
        "  fit['Ridge'] = mean(cross_val_score(model_ridge, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
        "  fit['AdaBoost'] = mean(cross_val_score(model_ada, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
        "  fit['GradBoost'] = mean(cross_val_score(model_gb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
        "  fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
        "\n",
        "  model['Logistic'] = model_log\n",
        "  model['KNN'] = model_knn\n",
        "  model['Ridge'] = model_ridge\n",
        "  model['AdaBoost'] = model_ada\n",
        "  model['GradBoost'] = model_gb\n",
        "  model['XGBoost'] = model_xgb\n",
        "\n",
        "  df_fit = pd.DataFrame({'Accuracy':fit})\n",
        "  df_fit = df_fit.sort_values(by=['Accuracy'], ascending=False)\n",
        "\n",
        "  print('Classification Models')\n",
        "  print(df_fit)\n",
        "\n",
        "  best_model = df_fit.index[0]\n",
        "  return model[best_model].fit(X, y)"
      ],
      "metadata": {
        "id": "pHboqDoc6LHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clustering_model_kmeans(df, num_clusters):   \n",
        "  import plotly.express as px\n",
        "  import sklearn.cluster as cluster\n",
        "  from scipy.spatial import distance as sdist\n",
        "\n",
        "  kmeans = cluster.KMeans(n_clusters=num_clusters, random_state=12345).fit(df)\n",
        "\n",
        "  df_w_cluster = df.copy()\n",
        "\n",
        "  df_w_cluster['cluster'] = kmeans.fit_predict(df)\n",
        "  \n",
        "  return kmeans"
      ],
      "metadata": {
        "id": "3PcfWDORBaEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommends about 5 clusters\n",
        "def clustering_model_test_cal_and_har(df):\n",
        "  import pandas as pd\n",
        "  import plotly.express as px\n",
        "  import seaborn as sns\n",
        "  import sklearn.cluster as cluster\n",
        "  from sklearn.cluster import KMeans\n",
        "  from scipy.spatial import distance as sdist\n",
        "  from sklearn.metrics import calinski_harabasz_score\n",
        "  from matplotlib import pyplot as plt\n",
        "  from sklearn.metrics import silhouette_score\n",
        "\n",
        "  ch_score = []\n",
        "  for n in range(2, 21):\n",
        "    kmeans = KMeans(n, random_state=12345).fit(df)\n",
        "    ch_score.append(calinski_harabasz_score(df, labels=kmeans.labels_))\n",
        "  \n",
        "  plt.plot(range(2, 21), ch_score, 'bx-')\n",
        "  plt.xlabel('number of clusters') \n",
        "  plt.ylabel('Calinski_Harabasz Criterion') \n",
        "  plt.title('Optimal Number of Clusters')\n",
        "  plt.text(12, 40, 'Higher is better', bbox=dict(facecolor='red', alpha=0.5))\n",
        "  plt.show()\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "5pEXgNUieLg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Recommends 2-3\n",
        "def clustering_model_test_sil(df):\n",
        "  from sklearn.metrics import silhouette_score\n",
        "  import pandas as pd\n",
        "  import plotly.express as px\n",
        "  import seaborn as sns\n",
        "  import sklearn.cluster as cluster\n",
        "  from sklearn.cluster import KMeans\n",
        "  from scipy.spatial import distance as sdist\n",
        "  from sklearn.metrics import calinski_harabasz_score\n",
        "  from matplotlib import pyplot as plt\n",
        "  from sklearn.metrics import silhouette_score\n",
        "  \n",
        "  si_score = []\n",
        "  for n in range(2, 21):\n",
        "    kmeans = KMeans(n, random_state=12345).fit(df)\n",
        "    si_score.append(silhouette_score(df, kmeans.labels_))\n",
        "  \n",
        "  plt.plot(range(2, 21), si_score, 'bx-')\n",
        "  plt.xlabel('number of clusters') \n",
        "  plt.ylabel('Silhouette score') \n",
        "  plt.title('Optimal Number of Clusters')\n",
        "  plt.text(11, .14, 'Higher is better', bbox=dict(facecolor='red', alpha=0.5))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "9JsIkjhkhnef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Recommends around 4\n",
        "def clustering_model_test_elbow(df):\n",
        "  from sklearn.metrics import silhouette_score\n",
        "  import pandas as pd\n",
        "  import plotly.express as px\n",
        "  import seaborn as sns\n",
        "  import sklearn.cluster as cluster\n",
        "  from sklearn.cluster import KMeans\n",
        "  from scipy.spatial import distance as sdist\n",
        "  from sklearn.metrics import calinski_harabasz_score\n",
        "  from matplotlib import pyplot as plt\n",
        "  from sklearn.metrics import silhouette_score\n",
        "\n",
        "  ss_score = []\n",
        "  for n in range(2,21):\n",
        "      kmeans = KMeans(n, random_state=12345).fit(df)\n",
        "      ss_score.append(kmeans.inertia_)\n",
        "      \n",
        "  # Where does the slope bend? Find the highest (least negative) slope.\n",
        "  changes = []\n",
        "  for n in range(2, 20):\n",
        "    changes.append(float(ss_score[n - 1] - ss_score[n - 2]))\n",
        "\n",
        "  optimal_n = changes.index(max(changes))\n",
        "\n",
        "  plt.plot(range(2,21), ss_score, 'bx-', markevery=[optimal_n])\n",
        "  plt.xlabel('number of clusters')\n",
        "  plt.ylabel('SS distance')\n",
        "  plt.title('Optimal Number of Clusters')\n",
        "  plt.text(8, 900, 'The point where slope \"bends\" from a \\ndecreasing to increasing rate of change', bbox=dict(facecolor='red', alpha=0.5))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "8nG0MHh4i06p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Save Model"
      ],
      "metadata": {
        "id": "rIcaN7YRcYqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dump_pickle(model, file_name):\n",
        "  import pickle\n",
        "  pickle.dump(model, open(file_name, \"wb\"))\n",
        "\n",
        "def load_pickle(file_name):\n",
        "  import pickle\n",
        "  model = pickle.load(open(file_name, \"rb\"))\n",
        "  return model"
      ],
      "metadata": {
        "id": "lAMxN9WocdFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 11: Function Calls\n"
      ],
      "metadata": {
        "id": "fyN9oWaTWEq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You may need to uncomment cell 1 and install the necessary packages, then restart runtime, comment those lines out again, then run the whole thing\n",
        "\n",
        "df = collectData(url='https://the-sneaker-database.p.rapidapi.com/sneakers', numberOfRows=1350, brand=['jordan']) #Must pull in more than 550 rows because of missing values\n",
        "df = image_analytics(df)\n",
        "\n",
        "# Collecting data and the image analytics can take about 1 hour to run. \n",
        "# If you would prefer not to wait for that, use the finalProjectImageData.csv\n",
        "# This csv has just the raw data collected and also the image analytics pulled from Imagga\n",
        "# Make sure to set 'name' as the new ID if you pull this yourself\n",
        "# You can also uncomment this next function, comment the first two lines, and it will pull from that csv\n",
        "# df = collectData2()\n",
        "\n",
        "# Clean Data\n",
        "df = bin_gender(df, col='gender', percent=0.05)\n",
        "df = bin_silhouette(df, col='silhouette', percent=0.04)\n",
        "df = fix_dates(df, col='releaseDate')\n",
        "df = fix_skewness(df)\n",
        "df = text_analytics(df)\n",
        "df = sentiment_calc(df)\n",
        "df = word_count(df)\n",
        "df = identify_entities(df)\n",
        "\n",
        "df_dummy_codes_market_value = impute_reg(df, 'estimatedMarketValue')\n",
        "df_features_market_value = feature_selection_variance(df_dummy_codes_market_value, 'estimatedMarketValue', p=0.8)\n",
        "model_reg = fit_crossvalidate_reg(df_features_market_value, 'estimatedMarketValue')\n",
        "dump_pickle(model_reg, 'best_reg_model.sav')\n",
        "\n",
        "df_dummy_codes_gender = impute_reg(df, 'gender')\n",
        "df_features_gender = feature_selection_variance(df_dummy_codes_gender, 'gender', p=0.8)\n",
        "model_clf = fit_crossvalidate_clf(df_features_gender, 'gender')\n",
        "dump_pickle(model_clf, 'best_clf_model.sav')\n",
        "\n",
        "# This commented code is for discovering the best number of clusters for our cluster model\n",
        "# clustering_model_test_cal_and_har(df_dummy_codes_market_value)\n",
        "# clustering_model_test_sil(df_dummy_codes_market_value)\n",
        "# clustering_model_test_elbow(df_dummy_codes_market_value)\n",
        "\n",
        "model_cluster = clustering_model_kmeans(df_dummy_codes_market_value, num_clusters=3)\n",
        "dump_pickle(model_cluster, 'best_cluster_model.sav')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrGtmEGucN4P",
        "outputId": "686c167e-b733-4cd5-94d6-dd8219ebb4df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning:\n",
            "\n",
            "For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "Regression Models\n",
            "          R-squared\n",
            "XGBoost    0.480028\n",
            "AdaBoost   0.389299\n",
            "Ridge      0.278936\n",
            "MLR        0.278002\n",
            "Bayesian   0.274442\n",
            "Lasso      0.270273\n",
            "Inverse   -0.034402\n",
            "Poisson   -0.034402\n",
            "[07:09:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Classification Models\n",
            "           Accuracy\n",
            "XGBoost    0.753298\n",
            "GradBoost  0.743668\n",
            "Logistic   0.633203\n",
            "AdaBoost   0.614218\n",
            "KNN        0.550476\n",
            "Ridge      0.548182\n"
          ]
        }
      ]
    }
  ]
}